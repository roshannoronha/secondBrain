# Healthcare and Social Services

## Overview

If done incorrectly automation can affect how we received or qualify for health care and social services.

1) [Algorithm cutting off healthcare](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy
) 

Notes

- Tammy has CP and her home care hours were cut in half (60 - 30 hrs). This extended to hundreds of people in Arkansaw and they all had similar situations
- Hours were changed due to a change in an algorithm
- Algorithmic tools are used in everything from healthcare to law enforcement.
	- These algorithms are complex in scope which means that it's beyond the understanding of the general public.
	- In healthcare they are used since they are cheaper to implement that hiring staff
- When issues do occur, algorithms are typically not disclosed as they are considered "propriatery".
- 60 inputs factored into determining the number of hours CP paitents qualified for.
	- Based on those inputs the algorithm would sort people into different categories
	- Some inputs mattered quite a bit and a difference between 3 and 4 could result in significantly reduced hours
	- Errors also due to how data was collected i.e. an amuputee with no leg had "no food problems"
	- A human was unable to correctly categorize a patient
- Algorithm ended up using the wrong calculation although this was only discovered in court.
	- Didn't account for issues due to diabetes as well


- Hard to be aware of an issue if no alert occurs and if the people that implement it aren't aware something is wrong.
- There is a natural trust that algorithms will be unbiased and neutral but the reality is that due to poor implementation this is usually not the case
- 


- In Colorado, programmers [embedded over nine hundred incorrect rules into Coloradoâ€™s public benefits system.123 With one such incorrect rule, CBMS denied Medicaid to patients with breast and cervical cancer based on income and asset limits that were not authorized by federal or state law.](https://deliverypdf.ssrn.com/delivery.php?ID=197095009065068126000076065104114028041051023052059052078008118071071112074102070069035037056044123004055021005026004009080004103027053013003021101001030004124076001082094002081001071119025069092068109081084089008030096099098093099088126113011119004&EXT=pdf&INDEX=TRUE)
- 




Takeaways

1) The datasets used to train health care software can have existing biases. This means that 
2) The people in charge of rolling out these algrithms may not have enough context to see if the algorithm is evaluating things correctly.
3) The programmers may not have enough context for the topic they are programming.
4) Removes the human element so individual cases/exemptions may not be considered.
5) Tend to be a black box. No one is sure how a decision is reached since numerous parameters are used to make the final calculation







[[What is an Algorithm]]
[[What is Bias]]
#socialIssue 
